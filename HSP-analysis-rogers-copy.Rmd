---
title: "final_analysis"
output:
  html_document: default
  pdf_document: default
date: "2023-10-25"
---

```{r}
library(tidyverse)
library(lmerTest)
library(lme4)
library(brms)
library(ggmcmc)
library(mcmcplots)
library(rstanarm)
library(RColorBrewer) # needed for some extra colours in one of the graphs
library(bayesplot)
library(ggplot2)
library(dplyr)
library(tidybayes)
library(modelr)
```

```{r}
spelled_out_conditions <- c("correct"="Correct Image","no"="No Image","wrong"="Wrong Image")
dataset = read_csv("./final_study_all_data_v2.csv",
                   col_types = cols(Condition_ID=col_factor(levels=c("no", "wrong", "correct")),
                                    POS = col_factor(levels = c("Closed","Open")))
                   )
dataset$POS <- factor(dataset$POS)
dataset$Condition_ID <- factor(dataset$Condition_ID)
dataset$Subject_ID <- factor(dataset$Subject_ID)
dataset$Group <- factor(dataset$Group)
dataset$WordToken <- factor(dataset$WordToken)
dataset$Word <- factor(dataset$Word)

sentence_counts <- dataset %>%
  group_by(Subject_ID, Condition_ID) %>%
  summarise(sentences_count = n_distinct(Sentence))

# Calculate the error rate using the formula
error_rates <- sentence_counts %>%
  mutate(error_rate = (12 - sentences_count) / 12)

print(error_rates)
model <- lmer(error_rate ~ Condition_ID + (1|Subject_ID), data = error_rates)
summary(model)

filtered_data11 <- dataset %>% filter(Condition_ID!='wrong')
filtered_data11$Condition_ID <- relevel(filtered_data11$Condition_ID, ref = "correct")
filtered_data11$POS <- relevel(filtered_data11$POS, ref = "Open")
filtered_data11$Condition_ID <- droplevels(filtered_data11$Condition_ID)
contrasts(filtered_data11$Condition_ID) <- "contr.sum"
contrasts(filtered_data11$POS) <- "contr.sum"
```
```{r}
filtered_data12 <- dataset %>% filter(Condition_ID!='no')
filtered_data12$Condition_ID <- relevel(filtered_data12$Condition_ID, ref = "correct")
filtered_data12$POS <- relevel(filtered_data12$POS, ref = "Open")
filtered_data12$Condition_ID <- droplevels(filtered_data12$Condition_ID)
contrasts(filtered_data12$Condition_ID) <- "contr.sum"
contrasts(filtered_data12$POS) <- "contr.sum"
```


```{r}
# Seeing the effects of Condition_ID for poorly grounded words to see if there's facilitation. 
# Considering slow-downs for correct vs no and correct vs wrong conditions
filtered_data2 <- dataset %>% filter(POS=='Closed')
filtered_data2$POS <- droplevels(filtered_data2$POS)
filtered_data2$Condition_ID <- relevel(filtered_data2$Condition_ID, ref = "correct")
contrasts(filtered_data2$Condition_ID)

```
# Sentence level error rate
```{r}
error_rates <- read_csv("error_ind.csv")
system.time(m_errors <- glmer(correctness ~ Condition_ID + (Condition_ID | Sentence) + (Condition_ID | Subject_ID), family="binomial",data=error_rates))
system.time(m0_errors <- glmer(correctness ~ 1 + (Condition_ID | Sentence) + (Condition_ID | Subject_ID), family="binomial",data=error_rates))
summary(m_errors)
anova(m0_errors,m_errors)
error_rates_by_condition <-
  error_rates %>%
  group_by(Condition_ID,Subject_ID) %>%
  summarize(correct=mean(correctness)) %>%
  group_by(Condition_ID) %>%
  summarize(accuracy=mean(correct),accuracy_se=se(correct))

my_dodge <- position_dodge(0.9)
ggplot(error_rates_by_condition,aes(x=Condition_ID,y=accuracy,fill=Condition_ID)) +
  geom_bar(stat="identity", 
           position=my_dodge) +
  geom_errorbar(aes(ymin=accuracy-accuracy_se, ymax=accuracy+accuracy_se), width=.2,
                 position=position_dodge(.9)) 

```

# Analysis With BRMS model

```{r}
system.time(model1_brms <- brm(RT ~ Condition_ID*POS + gpt2_surp + Frequency + Length + (Condition_ID*POS + gpt2_surp | Subject_ID) +
              (Condition_ID | Group) + (Condition_ID | Word) + (Condition_ID | WordToken), 
              data=filtered_data11, warmup = 1000, iter = 4000, 
                          cores = 6, #chains = 2, 
              control = list(adapt_delta = 0.85),
                          seed = 123))
```



```{r}
summary(model1_brms)
contrasts(filtered_data11$Condition_ID)
contrasts(filtered_data11$POS)
```


Down below i am checking the PSRF values from the Gelman-Rubin Diagnostic (using the within and between chain variability).
You should look at the Upper CI/Upper limit, which are all should be close to 1.

```{r}
model1posterior <- as.mcmc(model1_brms)
gelman.diag(model1posterior[, 1:5])
```
Also plotting the geweke diagnostics to make sure the first and last parts of each chain is below the z-score line

```{r}
geweke.plot(model1posterior[, 1:5])
```

The plots look good, we should look at the traceplots associated with the effects we care about to make sure the chains look 
like they mixed well. 

```{r}
library(bayesplot)
width <- 8  
height <- 6  # Height in inches
plot(model1_brms, plotfun = "trace", width=width, height=height)
```

Looks like the chains mixed well, no divergent chains visible in any random or fixed effects
We should also look at the pp_check to make sure it makes sense and no further adjustment of prior is needed

```{r}
pp_check(model1_brms)
```
I think it looks okay, the model is acceptable

```{r}
model1transformed <- ggs(model1_brms) # the ggs function transforms the BRMS output into a longformat tibble, that we can use to make different types of plots.
```

Finally drawing the plots of the posterior distributions to make sure they look normal

```{r}
ggplot(filter(model1transformed, Parameter %in% c("b_Condition_ID1","b_Condition_ID1:POS1"), 
              Iteration > 1000),
       aes(x    = value,
           fill = Parameter))+
    geom_density(alpha = .5)+
    geom_vline(xintercept = 0,
             col        = "red",
             size       = 1) +
  scale_x_continuous(name   = "Value",
                     limits = c(-80, 10))+ 
  
    geom_vline(xintercept = unlist(summary(model1_brms)$fixed[2,3:4]), col = "darkgreen", linetype = 2) +

    geom_vline(xintercept = unlist(summary(model1_brms)$fixed[7,3:4]), col = "blue",       linetype = 2) +
  theme_light()+
   scale_fill_manual(name   =  'Parameters', 
                     values = c("darkgreen" , "blue"), 
                     labels = c(expression( " "  ~  gamma[Condition_ID1]), 
                                expression( " "  ~  gamma[Condition_ID1:POS1])))+
  labs(title = "Figure 2: Posterior Density of Eq. 1 model parameters with 95% CI lines")

```


```{r}
saveRDS(model1_brms, 'final_11_brms.rds')
```

Repeating the same analysis but with correct vs wrong 


```{r}
system.time(model12_brms <- brm(RT ~ Condition_ID*POS + gpt2_surp + Frequency + Length + (Condition_ID*POS + gpt2_surp | Subject_ID) +
              (Condition_ID | Group) + (Condition_ID | Word) + (Condition_ID | WordToken), 
              data=filtered_data12, warmup = 1000, iter = 4000, 
                          cores = 6, chains = 2, control = list(adapt_delta = 0.85),
                          seed = 123))
```

```{r}
summary(model12_brms)
```
```{r}
pp_check(model12_brms,ndraws = 100)
```

Here I am trying to explore the third question, if facilitation is still observable for poorly grounded words, 
or closed class words, using all data from closed-class words

```{r}
model2_brms = brm(RT ~ Condition_ID + gpt2_surp + Frequency + Length + (Condition_ID + gpt2_surp | Subject_ID)
             + (Condition_ID | Group) + (Condition_ID | Word) + (Condition_ID | WordToken),
             data=filtered_data2,warmup = 1000, iter = 4500, 
                          cores = 6, chains = 2, control = list(adapt_delta = 0.85),
                          seed = 123)
```

```{r}
summary(model2_brms)
```
```{r}
model2transformed <- ggs(model2_brms)
```
```{r}
ggplot(filter(model2transformed, Parameter %in% c("b_Condition_IDwrong","b_Condition_IDno"), 
              Iteration > 1000),
       aes(x    = value,
           fill = Parameter))+
    geom_density(alpha = .5)+
    geom_vline(xintercept = 0,
             col        = "red",
             size       = 1) +
  scale_x_continuous(name   = "Value",
                     limits = c(-50, 80))+ 
        geom_vline(xintercept = unlist(summary(model2_brms)$fixed[2,3:4]), col = "red", linetype = 2) +
    geom_vline(xintercept = unlist(summary(model2_brms)$fixed[3,3:4]), col = "darkgreen", linetype = 2) +
  theme_light()+
   scale_fill_manual(name   =  'Parameters', 
                     values = c( "red","darkgreen"), 
                     labels = c(expression( " "  ~  gamma[Condition_IDno]), 
                                expression( " "  ~  gamma[Condition_IDwrong])))+
  labs(title = "Figure 3: Posterior Density of Eq. 2. model parameters with 95% CI lines")
```


```{r}
model2_brms <- readRDS("saved_models/model2_brms.rds")
summary(model2_brms)
model11_brms <- readRDS("saved_models/model11_brms.rds")
summary(model11_brms)
model_blip_brm <- readRDS("saved_models/model_blip_brm.rds")
summary(model_blip_brm)
```

```{r}
rts_by_word_token <-
  dataset %>%
  group_by(WordToken,Condition_ID,Group,POS) %>%
  summarize(RT=mean(RT)) %>%
  group_by(Condition_ID,Group,POS) %>%
  summarize(RT=mean(RT)) %>%
  group_by(Condition_ID,POS) %>%
  summarize(rt=mean(RT),rt_se=se(RT)) %>%
  mutate(Condition=spelled_out_conditions[as.character(Condition_ID)],
         `Part of Speech`=POS)
my_dodge <- position_dodge(0.9)  
ggplot(rts_by_word_token,aes(x=Condition,y=rt,fill=`Part of Speech`)) +
    geom_bar(stat="identity", 
           position=my_dodge) +
  geom_errorbar(aes(ymin=rt-rt_se, ymax=rt+rt_se), width=.2,
                 position=position_dodge(.9)) +
  ylab("RT Â± Standard Error") +
  ylim(c(0,1200)) +
  theme_bw() +
  theme(legend.position = c(0.4,0.91),
        legend.direction="horizontal")
#ggsave("/tmp/RT-by-condition-and-POS.png",height=4,width=4)
ggsave("/tmp/RT-by-condition-and-POS.pdf",height=3,width=4)
```
```{r}
dat_pos <- 
  dataset %>%
  group_by(WordToken,Subject_ID,Group,Condition_ID) %>%
  summarize(POS=POS[1],gpt2_surp=mean(gpt2_surp),blip2_surp=mean(blip2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT))
dat_pos_by_word_token <-
  dat_pos %>%
  group_by(WordToken,Group,Condition_ID) %>%
  summarize(POS=POS[1],gpt2_surp=mean(gpt2_surp),blip2_surp=mean(blip2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT))
d_pos_correct_wrong <- filter(dat_pos_by_word_token,Condition_ID != "no")
d_pos_correct_wrong$pos <- ifelse(d_pos_correct_wrong$POS=="Open",1,-1)
d_pos_correct_wrong$Condition <- ifelse(d_pos_correct_wrong$Condition_ID=="correct",1,-1)
m_pos_correct_wrong <- lmer(RT ~ POS/Condition + gpt2_surp + Frequency + Length + (POS/Condition + gpt2_surp + Frequency + Length | Group) + (1|WordToken),data=d_pos_correct_wrong,REML=F)
m_pos_correct_wrong2 <- lmer(RT ~ POS*Condition + gpt2_surp + Frequency + Length + (POS*Condition + gpt2_surp + Frequency + Length | Group) + (1|WordToken),data=d_pos_correct_wrong,REML=F)
m_pos_correct_wrong <- lmer(RT ~ POS/Condition + blip2_surp + Frequency + Length + (POS/Condition + gpt2_surp + Frequency + Length | Group) + (1|WordToken),data=d_pos_correct_wrong,REML=F)
summary(m_pos_correct_wrong)
summary(m_pos_correct_wrong2)
d_pos_correct_no <- filter(dat_pos_by_word_token,Condition_ID != "wrong")
d_pos_correct_no$pos <- ifelse(d_pos_correct_no$POS=="Open",1,-1)
d_pos_correct_no$Condition <- ifelse(d_pos_correct_no$Condition_ID=="correct",1,-1)
m_pos_correct_no <- lmer(RT ~ POS/Condition + gpt2_surp + Frequency + Length + (POS/Condition + gpt2_surp + Frequency + Length | Group) + (1|WordToken),data=d_pos_correct_no,REML=F)
m_pos_correct_no2 <- lmer(RT ~ POS*Condition + gpt2_surp + Frequency + Length + (POS*Condition + gpt2_surp + Frequency + Length | Group) + (1|WordToken),data=d_pos_correct_no,REML=F)
summary(m_pos_correct_no)
summary(m_pos_correct_no2)
d_pos_wrong_no <- filter(dat_pos_by_word_token,Condition_ID != "correct")
d_pos_wrong_no$pos <- ifelse(d_pos_wrong_no$POS=="Open",1,-1)
d_pos_wrong_no$Condition <- ifelse(d_pos_wrong_no$Condition_ID=="no",1,-1)
m_pos_wrong_no <- lmer(RT ~ POS/Condition + gpt2_surp + Frequency + Length + (POS/Condition + gpt2_surp + Frequency + Length | Group) + (1|WordToken),data=d_pos_wrong_no,REML=F)
summary(m_pos_wrong_no)
```
```{r}
rt_word_token_by_condition <- 
  dataset %>%
  group_by(WordToken,groundedness,POS,Condition_ID) %>%
  summarize(RT=mean(RT)) %>%
  pivot_wider(names_from=Condition_ID,values_from=RT) %>%
  mutate(`Relative to Wrong Image`=wrong-correct,
         `Relative to No Image`=no-correct) %>%
  pivot_longer(cols=c(`Relative to Wrong Image`,`Relative to No Image`),names_to="Relative to",values_to="RT_diff")
ggplot(rt_word_token_by_condition,aes(x=groundedness,y=RT_diff,color=POS)) +
  geom_point(alpha=0.2) + 
  stat_smooth(method="lm") + 
  theme_bw()  + 
  #xlim(c(-10,10)) + 
  coord_cartesian(ylim=c(-500,500)) +
  ylab("RT advantage for correct image preview") + 
  theme(legend.position=c(0.08,0.17)) +
  facet_grid(.~`Relative to`)
#ggsave("/tmp/RT_advantage_by_groundedness.png",height=3.5,width=6)
ggsave("/tmp/RT_advantage_by_groundedness.pdf",height=3.5,width=6)
```
```{r}
dat_grounding <- 
  dataset %>%
  filter(Condition_ID != "no") %>%
  group_by(WordToken,Subject_ID,Group,Condition_ID) %>%
  summarize(POS=POS[1],gpt2_surp=mean(gpt2_surp),blip2_surp=mean(blip2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT))
dat_grounding_by_word_token <-
  dat_grounding %>%
  group_by(WordToken,Group,Condition_ID) %>%
  summarize(POS=POS[1],gpt2_surp=mean(gpt2_surp),blip2_surp=mean(blip2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT)) %>%
  mutate(#groundedness=(10+groundedness)/2,
         blip2_gpt2_surp_diff=blip2_surp-gpt2_surp,
         Condition=ifelse(Condition_ID=="wrong","Wrong Image Preview","Correct Image Preview"),
         Cond=ifelse(Condition_ID=="wrong",-1,1),
         `Part of Speech`=POS,
         pos=ifelse(POS=="Open",1,-1),
         contingent_surp=ifelse(Condition_ID=="wrong",gpt2_surp,blip2_surp))
summary(lm(blip2_gpt2_surp_diff ~ `Part of Speech` : groundedness, filter(dat_grounding_by_word_token, Condition_ID=="correct")))
ggplot(dat_grounding_by_word_token,aes(x=groundedness,y=blip2_gpt2_surp_diff,color=`Part of Speech`)) +
  geom_point(alpha=0.2) +
  stat_smooth(method="lm") + 
  theme_bw() +
  #lim(c(-10,10)) +
  xlab("completely ungrounded          <-        neutral       ->        completely grounded\nGroundedness of Word in Correct Image") +
  ylab("BLIP-2 surprisal - GPT-2 surprisal") +
  coord_cartesian(ylim=c(-10,5)) +
  theme(legend.position = c(0.65, 0.175)) +
  facet_grid(.~Condition) 
#ggsave("/tmp/surprisal_diff.png",height=4.51*3/4,width=7.29*3/4)
ggsave("/tmp/surprisal_diff.pdf",height=4.51*4/5,width=7.29*4/5)
dat_grounding_by_word_token %>%
  group_by(Condition_ID) %>%
  summarize(surp_diff=mean(blip2_gpt2_surp_diff))
t.test(filter(dat_grounding_by_word_token,Condition_ID=="wrong")$blip2_gpt2_surp_diff)
t.test(filter(dat_grounding_by_word_token,Condition_ID=="correct")$blip2_gpt2_surp_diff)
blip2_surp_by_condition <- 
  dat_grounding_by_word_token %>%
  select(WordToken,Condition_ID,blip2_surp) %>%
  pivot_wider(names_from=Condition_ID,values_from=blip2_surp)


ggplot(blip2_surp_by_condition,aes(x=wrong,y=correct)) +
  geom_point() +
  geom_abline(intercept=0,slope=1)
ggsave("/tmp/blip2_surp_by_condition.pdf")
system.time(m_blip2 <- lmer(RT ~ Condition_ID*groundedness + Condition_ID/blip2_surp + Frequency + Length + (Condition_ID*groundedness + Condition_ID/blip2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_blip2)
system.time(m_groundedness <- lmer(RT ~ Condition_ID/groundedness + gpt2_surp + Frequency + Length + (Condition_ID/groundedness + gpt2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_groundedness)

system.time(m_gpt2 <- lmer(RT ~ Condition_ID*groundedness + Condition_ID/gpt2_surp + Frequency + Length + (Condition_ID*groundedness + Condition_ID/gpt2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_gpt2)
system.time(m_both <- lmer(RT ~ Condition_ID*groundedness + Condition_ID/gpt2_surp + Condition_ID/blip2_surp+ Frequency + Length + (Condition_ID*groundedness + Condition_ID/gpt2_surp + Condition_ID/blip2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_both)
system.time(m_contingent <- lmer(RT ~ Condition_ID*groundedness + Condition_ID/contingent_surp + Frequency + Length + (Condition_ID*groundedness + Condition_ID/contingent_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_contingent)
system.time(m_blip2_POS_and_groundedness <- lmer(RT ~ Condition_ID*(groundedness+POS) + Condition_ID/blip2_surp + Frequency + Length + (Condition_ID*(groundedness+POS) + Condition_ID/blip2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_blip2_POS_and_groundedness)

system.time(m_gpt2_full <- lmer(RT ~ Condition/(groundedness + pos) + Condition:gpt2_surp + Frequency + Length + (Condition*(groundedness + pos) + Condition:gpt2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_gpt2_full)
logLik(m_gpt2_full)
system.time(m_blip2_full <- lmer(RT ~ Condition/(groundedness + pos) + Condition:blip2_surp + Frequency + Length + (Condition*(groundedness + pos) + Condition:blip2_surp + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_blip2_full)
logLik(m_blip2_full)
m_gpt2_coef <- as.data.frame(coef(summary(m_gpt2_full))) %>%
  mutate(model="GPT-2")
m_gpt2_coef$coefficient <- rownames(m_gpt2_coef)
m_blip2_coef <- as.data.frame(coef(summary(m_blip2_full))) %>%
  mutate(model="BLIP-2")
m_blip2_coef$coefficient <- rownames(m_blip2_coef)
coefficient_renames <- c("ConditionCorrect Image Preview:groundedness"="Groundedness Effect with Correct Image (ms/rating point)",
                         "ConditionWrong Image Preview:groundedness"="Groundedness Effect with Wrong Image (ms/rating point)",
                         "ConditionCorrect Image Preview:gpt2_surp"="Surprisal effect with Correct Image (ms/bit)",
                         "ConditionWrong Image Preview:gpt2_surp"="Surprisal effect with Wrong Image (ms/bit)",
                         "ConditionCorrect Image Preview:blip2_surp"="Surprisal effect with Correct Image (ms/bit)",
                         "ConditionWrong Image Preview:blip2_surp"="Surprisal effect with Wrong Image (ms/bit)",
                         "Length"="Length (ms/character)",
                         "Frequency"="Frequency (ms/bit)")
coefs <- rbind(m_gpt2_coef,m_blip2_coef) %>%
  filter(coefficient %in% names(coefficient_renames)) %>%
  mutate(coefficient=coefficient_renames[coefficient])
my_dodge <- position_dodge(0.9)
ggplot(coefs,aes(x=coefficient,y=Estimate,fill=model)) +
    geom_bar(stat="identity", 
           position=my_dodge) +
  scale_fill_manual(values = c("green","magenta")) +
  geom_errorbar(aes(ymin=Estimate - 1.97*`Std. Error`, ymax=Estimate + 1.97*`Std. Error`), width=.2,
                 position=position_dodge(.9)) +
  coord_flip() +
  theme_bw() +
  ylab("Coefficient estimate & 95% CI") +
  xlab("") +
  theme(legend.position=c(0.81,0.26),
        legend.title=element_blank())
#ggsave("/tmp/coefs_with_blip2_and_gpt2.png",height=2,width=7)
ggsave("/tmp/coefs_with_blip2_and_gpt2.pdf",height=2,width=6)
system.time(m_both_full <- lmer(RT ~ Condition/(groundedness + pos) + Condition:(blip2_surp + gpt2_surp) + Frequency + Length + (Condition*(groundedness + pos) + Condition:(blip2_surp + gpt2_surp) + Frequency + Length | Group), dat_grounding_by_word_token,REML=F))
summary(m_both_full)
```
