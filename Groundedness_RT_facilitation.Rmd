---
title: "Analysis of Groundedness, RT facilitation and Surprisal Difference"
output: html_document
date: "2024-01-31"
---

```{r}
library(lmerTest)
library(lme4)
library(brms)
library(ggmcmc)
library(mcmcplots)
library(rstanarm)
library(RColorBrewer) # needed for some extra colours in one of the graphs
library(bayesplot)
library(ggplot2)
library(dplyr)
library(tidybayes)
library(modelr)
library(tidyverse)
```

Loading the dataset here, correctness in that dataset has 3 levels, correct, wrong and unavailable, check the Readme to learn more about final_v2_all_with_error_info.csv

```{r}
dataset_with_error <- read_csv("./final_v2_all_with_error_info.csv",
                   col_types = cols(Condition_ID=col_factor(levels=c("no", "wrong", "correct")),
                                    POS = col_factor(levels = c("Closed","Open")))
                   ) %>% filter(correctness != 'unavailable') 
```



# Surprisal Difference(different types of surprisals) & groundedness

In this section, I am trying to understand the relationship between surprisal difference and groundedness, between a mLLM and LLM, in the correct condition only, because the two other conditions don't make sense particularly. Assuming the relationship is linear(which could be relaxed later and we could fit a gam model), I have fit a lmer model below to see the if the linear relation is significant. What I found is that the relation is only significant when the words are open class words.

```{r}
dataset_correct <- dataset_with_error %>% filter(correctness!='wrong')
dat_grounding <- 
  dataset_correct %>%
  filter(Condition_ID != "no") %>%
  group_by(WordToken,Subject_ID,Group,Condition_ID) %>%
  summarize(POS=POS[1],gpt2_surp=mean(gpt2_surp),kosmos2_surp=mean(kosmos2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT))
dat_grounding_by_word_token <-
  dat_grounding %>%
  group_by(WordToken,Group,Condition_ID) %>%
  summarize(POS=POS[1],gpt2_surp=mean(gpt2_surp),kosmos2_surp=mean(kosmos2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT)) %>%
  mutate(kosmos2_gpt2_surp_diff=kosmos2_surp-gpt2_surp,
         Condition=ifelse(Condition_ID=="wrong","Wrong Image Preview","Correct Image Preview"),
         `Part of Speech`=POS)

model_surp_diff <- lmer(kosmos2_gpt2_surp_diff ~ 1 + `Part of Speech`:groundedness +  Length + Frequency + (`Part of Speech`:groundedness +  Length + Frequency | Group) , filter(dat_grounding_by_word_token, Condition_ID=='correct'))

summary(model_surp_diff)
```

# Surprisal Difference (Same type) & groundedness
In this section, I am trying the same, except that the surprisal difference here is coming from the same model(I am looking at the difference of surprisal between two conditions of kosmos2, correct - no and seeing how groundedness is predictive of this difference), which is still important to look at since in the previous analysis, there could be other factors related to training regime and training dataset difference between two models that could be contribute to the result. The finding is very interesting here, but the effect of groundedness on surprisal difference is significant for both open and closed class words. (Question: I only put the interaction between open and closed class words, not the main effects of groundedness and POS, is that good or bad?)

```{r}
dat_grounding <- 
  dataset_correct %>%
  filter(Condition_ID != "wrong") %>%
  group_by(WordToken,Subject_ID,Group,Condition_ID) %>%
  summarize(POS=POS[1],kosmos2_surp=mean(kosmos2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT))

dat_grounding_by_word_token <-
  dat_grounding %>%
  group_by(WordToken,Group,Condition_ID) %>%
  summarize(POS=POS[1],kosmos2_surp=mean(kosmos2_surp),Frequency=mean(Frequency),Length=mean(Length),groundedness=mean(groundedness),RT=mean(RT)) # %>%
  #pivot_wider(names_from = Condition_ID, values_from = kosmos2_surp) ##%>%
  #mutate(#groundedness=(10+groundedness)/2,
         #kosmos2_surp_diff= wrong-correct,
         #`Part of Speech`=POS)

dat_grounding_by_word_token_correct <- dat_grounding_by_word_token %>% filter(Condition_ID=='correct') %>% select(-Condition_ID) 
dat_grounding_by_word_token_no <- dat_grounding_by_word_token %>% filter(Condition_ID=='no') %>% select(-Condition_ID)

combined_df <- inner_join(dat_grounding_by_word_token_correct, dat_grounding_by_word_token_no, by = c("WordToken","Group","POS","groundedness","Frequency","Length")) %>%
  mutate(kosmos2_surp_diff = kosmos2_surp.x - kosmos2_surp.y) %>%
  mutate(RT_diff = RT.x - RT.y) %>%
  select(-starts_with("kosmos2_surp.")) %>%
  select(-starts_with("RT."))

ggplot(combined_df,aes(x=groundedness,y=kosmos2_surp_diff,color=POS)) +
  geom_point() +
  stat_smooth(method="lm") + 
  theme_bw() +
  xlim(c(-5,10)) +
  ylim(c(-20,7)) +
  xlab("Groundedness of Word in Correct Image") +
  ylab("KOSMOS-2 surp diff(corrrect - no)") +
  theme(legend.position = "top") #+
  #facet_grid(.~Condition) 

model_surp_diff_same <- lmer(kosmos2_surp_diff ~ 1 + POS:groundedness +  Length + Frequency + (POS:groundedness +  Length + Frequency | Group) , data=combined_df)
```

```{r}
summary(model_surp_diff_same)
```

# RT facilitation & surprisal, groundedness
The goal here is to understand how reaction time facilitation(no - correct & wrong - correct) relates to surprisal difference(no - correct & wrong - correct of the same model) and groundedness. Looks like both surprisal difference & groundedness score(when the word is from open class specially) has significat effect on RT facilitation, specially for well grounded or open class words


```{r}
dataset_with_error$RT <- as.numeric(dataset_with_error$RT)
all_correct <- dataset_with_error %>% filter(correctness=="correct")
rt_word_token_by_condition1 <- 
  all_correct %>%
  group_by(WordToken,groundedness,POS,Condition_ID, Group, Frequency, Length) %>%
  summarize(RT=mean(RT)) %>%
  pivot_wider(names_from=Condition_ID,values_from=RT) %>%
  mutate(`RT Diff to Wrong Image`=wrong-correct,
         `RT Diff to No Image`= no-correct) %>%
  pivot_longer(cols=c(`RT Diff to Wrong Image`,`RT Diff to No Image`),names_to="RT Diff to",values_to="RT_diff")

rt_word_token_by_condition2 <- 
  all_correct %>%
  group_by(WordToken,groundedness,POS,Condition_ID, Group, Frequency, Length) %>%
  summarize(kosmos2_surp=mean(kosmos2_surp)) %>%
  pivot_wider(names_from=Condition_ID,values_from=kosmos2_surp, names_prefix = "kosmos2surp_") %>%
  mutate(`Surp Diff to Wrong Image`=kosmos2surp_wrong-kosmos2surp_correct,
         `Surp Diff to No Image`=kosmos2surp_no-kosmos2surp_correct) %>%
  pivot_longer(cols=c(`Surp Diff to Wrong Image`,`Surp Diff to No Image`),names_to="Surp Diff to",values_to="Surp_diff")

combined_df <- inner_join(rt_word_token_by_condition1,rt_word_token_by_condition2)

combined_df$`Surp Diff to` <- factor(combined_df$`Surp Diff to`)

combined_df_wrong <- combined_df %>% filter(`Surp Diff to`=="Surp Diff to Wrong Image")
model_wrong <- lmer(RT_diff ~ Surp_diff + groundedness:POS + Frequency + Length + (Surp_diff + groundedness:POS + Frequency + Length | Group) + (1 | WordToken), data=combined_df_wrong)
summary(model_wrong)

combined_df_no <- combined_df %>% filter(`Surp Diff to`=="Surp Diff to No Image")
model_no <- lmer(RT_diff ~ Surp_diff + groundedness:POS + Frequency + Length + (Surp_diff + groundedness:POS + Frequency + Length | Group) + (1 | WordToken), data=combined_df_no)
summary(model_no)
```
# Error rate analysis 
In this section, I wanted to analyze the error rates and what is a good predictor of when people will make errors. On that note, I first analyzed the relationship between blip2 surprisal and error occurence 

```{r}
dataset_with_error_avg <- dataset_with_error %>% 
  group_by(WordToken,Condition_ID,Group,correctness) %>%
  summarize(blip2_surp=mean(blip2_surp)) %>%
  group_by(Condition_ID,Group,correctness) %>%
  summarize(blip2_surp=mean(blip2_surp)) %>%
  group_by(Condition_ID,correctness) %>%
  summarize(blip2_surp_avg=mean(blip2_surp),blip2_surp_se = sd(blip2_surp)/sqrt(n())) %>%
  mutate(Condition=spelled_out_conditions[as.character(Condition_ID)]) %>%
  filter(Condition_ID!='no')
  
```
```{r}
dataset_with_error_avg
dataset_with_error_avg <- dataset_with_error_avg %>% rename(`Correctness Status of Words`=correctness)
dataset_with_error_avg
```
```{r}
my_dodge <- position_dodge(0.9)  
ggplot(dataset_with_error_avg,aes(x=Condition,y=blip2_surp_avg,fill=`Correctness Status of Words`)) +
    geom_bar(stat="identity", 
           position=my_dodge,color = "black") +
  geom_errorbar(aes(ymin=blip2_surp_avg-blip2_surp_se, ymax=blip2_surp_avg+blip2_surp_se), width=.2,
                 position=position_dodge(.9)) +
  ylab("BLIP2_surprisal Â± Standard Error") +
  ylim(c(0,12)) +
  #scale_fill_manual(values = c("orange", "green"))
  theme_bw() +
  theme(legend.position = c(0.5,0.93),
        legend.direction="horizontal")
#ggsave("img/gpt2_error.pdf",height=4,width=4)
```

# Error prediction model, Correct Vs wrong condition data
I don't like running 3 models keeping data from two condition in each so that we can do sum encoding for POS and Condition_ID and understand the main effects for POS and Condition_ID. Is there an alternative to this?

```{r}
dataset_here <- dataset_with_error %>% filter(Condition_ID!='no')
dataset_here$Condition_ID <- relevel(dataset_here$Condition_ID, ref = "correct")
dataset_here$POS <- relevel(dataset_here$POS, ref = "Open")
dataset_here$Condition_ID <- droplevels(dataset_here$Condition_ID)
contrasts(dataset_here$Condition_ID) <- "contr.sum"
contrasts(dataset_here$POS) <- "contr.sum"

#m_errors <- brm(correctness ~ Condition_ID*POS + blip2_surp + (Condition_ID | Subject_ID) + (Condition_ID | Group) + (Condition_ID | WordToken), data=dataset_here, family = "bernoulli",warmup = 1000, iter = 4000,cores = 6, chains = 2, control = list(adapt_delta = 0.85),seed = 123)
```

# Bigger brms model causes convergence issues

```{r}
m_errors <- readRDS("saved_models/m_errors.RDS")
```

```{r}
m_errors_glmer <- glmer(correctness ~ Condition_ID*POS + kosmos2_surp + Frequency + Length + (Condition_ID*POS | Group) + (Condition_ID | Subject_ID) + (Condition_ID | WordToken) + (Condition_ID | Word), data=dataset_here, family="binomial",control = glmerControl(optimizer = "bobyqa"))
```



```{r}
summary(m_errors)
```

# Correct Vs no

```{r}

```

# No Vs Wrong 

I would like to know if there's a better way to do the above analysis without running 3 models 
```{r}

```
